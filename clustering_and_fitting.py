# -*- coding: utf-8 -*-
"""clustering_and_fitting.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1cbAvOhtmn3r0VPW6mHwKf-lVBtWacaJc

# Clustering
"""

!pip install wbgapi

import pandas as pd
import numpy as np
import wbgapi as wb
import matplotlib.pyplot as plt
from sklearn import preprocessing
from sklearn.cluster import KMeans
from scipy.optimize import curve_fit
from pandas.core.arrays import integer
from sklearn.preprocessing import MinMaxScaler
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import silhouette_score, silhouette_samples

def readData(region):
  data = wb.data.DataFrame(region, wb.region.members('EUU'), time=range(2005,2020) ,numericTimeKeys = True)
  data = pd.DataFrame(data)
  data = data.dropna()
  data_tp = data.T

  return [data, data_tp]


# Load data

[data, data_tp] = readData('SG.GEN.PARL.ZS')

data.head()

def map_corr(df, size=10):
    """Function creates heatmap of correlation matrix for each pair of 
    columns in the dataframe.

    Input:
        df: pandas DataFrame
        size: vertical and horizontal size of the plot (in inch)
        
    The function does not have a plt.show() at the end so that the user 
    can savethe figure.
    """

    import matplotlib.pyplot as plt  # ensure pyplot imported

    corr = df.corr()
    plt.figure(figsize=(size, size))
    # fig, ax = plt.subplots()
    plt.matshow(corr, cmap='coolwarm')
    # setting ticks to column names
    plt.xticks(range(len(corr.columns)), corr.columns, rotation=90)
    plt.yticks(range(len(corr.columns)), corr.columns)

    plt.colorbar()

map_corr(data.T)

# Normalize data
scaler = StandardScaler()
normalized_data = scaler.fit_transform(data)

# Perform clustering
kmeans = KMeans(n_clusters=3, random_state=0)
clusters = kmeans.fit_predict(normalized_data)

# Add cluster membership as a new column to the data frame
data['cluster'] = clusters

# Plot cluster membership and cluster centers
plt.scatter(data.index, data.iloc[:,0:1], c=data['cluster'])
plt.plot(kmeans.cluster_centers_[:, 0], label='Cluster centers')
plt.legend()
plt.xticks(rotation=90)
plt.show()

def err_ranges(x, func, param, sigma):
    """
    Calculates the upper and lower limits for the function, parameters and
    sigmas for single value or array x. Functions values are calculated for 
    all combinations of +/- sigma and the minimum and maximum is determined.
    Can be used for all number of parameters and sigmas >=1.
    
    This routine can be used in assignment programs.
    """

    import itertools as iter
    
    # initiate arrays for lower and upper limits
    lower = func(x, *param)
    upper = lower
    
    uplow = []   # list to hold upper and lower limits for parameters
    for p, s in zip(param, sigma):
        pmin = p - s
        pmax = p + s
        uplow.append((pmin, pmax))
        
    pmix = list(iter.product(*uplow))
    
    for p in pmix:
        y = func(x, *p)
        lower = np.minimum(lower, y)
        upper = np.maximum(upper, y)
        
    return lower, upper

from scipy.optimize import curve_fit

# Define a function to fit the data
def poly_func(x, a, b, c):
    return a + b * x + c * x**2

# Get the time values and the data to be fitted
time = [2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019]
ydata = data.iloc[26,:15]

# Fit the function to the data using curve_fit
popt, pcov = curve_fit(poly_func, time, ydata)

# Get the predicted values for the next 20 years
future_time = np.arange(2020, 2030)
y_pred = poly_func(future_time, *popt)

# Get the standard errors for the predicted values
stderr = np.sqrt(np.diag(pcov))

# Get the confidence ranges for the predicted values using err_ranges
conf_range = err_ranges(future_time, poly_func, popt, stderr)

# Plot the original data and the best-fitting curve with the confidence ranges
plt.scatter(time, ydata, label='Data')
plt.plot(future_time, y_pred, label='Best fit')
plt.fill_between(future_time, conf_range[0], conf_range[1], alpha=0.5, label='Confidence range')
plt.ylim(bottom=30)
plt.ylim(top=60)
plt.title('Forecast For Sweden')
plt.ylabel('Percentage of Women In Parliament')
plt.xlabel('Time in Years')
plt.legend()
plt.show()

# Get the time values and the data to be fitted
time = [2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019]
ydata = data.iloc[13,:15]

# Fit the function to the data using curve_fit
popt, pcov = curve_fit(poly_func, time, ydata)

# Get the predicted values for the next 20 years
future_time = np.arange(2020, 2030)
y_pred = poly_func(future_time, *popt)

# Get the standard errors for the predicted values
stderr = np.sqrt(np.diag(pcov))

# Get the confidence ranges for the predicted values using err_ranges
conf_range = err_ranges(future_time, poly_func, popt, stderr)

# Plot the original data and the best-fitting curve with the confidence ranges
plt.scatter(time, ydata, label='Data')
plt.plot(future_time, y_pred, label='Best fit')
plt.fill_between(future_time, conf_range[0], conf_range[1], alpha=0.5, label='Confidence range')
plt.legend()
plt.title('Forecast For Hungary')
plt.ylabel('Percentage of Women In Parliament')
plt.xlabel('Time in Years')
plt.ylim(bottom=5)
plt.ylim(top=30)
plt.show()